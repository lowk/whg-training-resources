"use strict";(self.webpackChunkwhg_training_resources=self.webpackChunkwhg_training_resources||[]).push([[783],{3905:(e,t,a)=>{a.d(t,{Zo:()=>u,kt:()=>c});var n=a(7294);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function r(e,t){if(null==e)return{};var a,n,s=function(e,t){if(null==e)return{};var a,n,s={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(s[a]=e[a]);return s}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(s[a]=e[a])}return s}var l=n.createContext({}),m=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=m(e.components);return n.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,s=e.mdxType,i=e.originalType,l=e.parentName,u=r(e,["components","mdxType","originalType","parentName"]),p=m(a),c=s,h=p["".concat(l,".").concat(c)]||p[c]||d[c]||i;return a?n.createElement(h,o(o({ref:t},u),{},{components:a})):n.createElement(h,o({ref:t},u))}));function c(e,t){var a=arguments,s=t&&t.mdxType;if("string"==typeof e||s){var i=a.length,o=new Array(i);o[0]=p;var r={};for(var l in t)hasOwnProperty.call(t,l)&&(r[l]=t[l]);r.originalType=e,r.mdxType="string"==typeof e?e:s,o[1]=r;for(var m=2;m<i;m++)o[m]=a[m];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},6833:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>d,frontMatter:()=>i,metadata:()=>r,toc:()=>m});var n=a(7462),s=(a(7294),a(3905));const i={sidebar_position:2},o="Meta-analysing two studies.",r={unversionedId:"genome_wide_association_studies/meta-analysis_and_fine-mapping/Meta-analysis",id:"genome_wide_association_studies/meta-analysis_and_fine-mapping/Meta-analysis",title:"Meta-analysing two studies.",description:"Up to the table of contents / Back to the Introduction / Forward to the fine-mapping section",source:"@site/docs/genome_wide_association_studies/meta-analysis_and_fine-mapping/Meta-analysis.md",sourceDirName:"genome_wide_association_studies/meta-analysis_and_fine-mapping",slug:"/genome_wide_association_studies/meta-analysis_and_fine-mapping/Meta-analysis",permalink:"/whg-training-resources/genome_wide_association_studies/meta-analysis_and_fine-mapping/Meta-analysis",draft:!1,editUrl:"https://github.com/whg-training/whg-training-resources/edit/main/docs/genome_wide_association_studies/meta-analysis_and_fine-mapping/Meta-analysis.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Introduction",permalink:"/whg-training-resources/genome_wide_association_studies/meta-analysis_and_fine-mapping/Introduction"},next:{title:"Using `FINEMAP` to fine-map associations",permalink:"/whg-training-resources/genome_wide_association_studies/meta-analysis_and_fine-mapping/Fine-mapping"}},l={},m=[{value:"Conducting fixed-effect meta-analysis",id:"conducting-fixed-effect-meta-analysis",level:3},{value:"Running the meta-analsis",id:"running-the-meta-analsis",level:3},{value:"Making a forest plot",id:"making-a-forest-plot",level:3},{value:"Other meta-analysis models",id:"other-meta-analysis-models",level:3},{value:"Fine-mapping the association",id:"fine-mapping-the-association",level:3}],u={toc:m};function d(e){let{components:t,...i}=e;return(0,s.kt)("wrapper",(0,n.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,s.kt)("h1",{id:"meta-analysing-two-studies"},"Meta-analysing two studies."),(0,s.kt)("p",null,(0,s.kt)("a",{parentName:"p",href:"/whg-training-resources/genome_wide_association_studies/meta-analysis_and_fine-mapping/"},"Up to the table of contents")," / ",(0,s.kt)("a",{parentName:"p",href:"/whg-training-resources/genome_wide_association_studies/meta-analysis_and_fine-mapping/Introduction"},"Back to the Introduction")," / ",(0,s.kt)("a",{parentName:"p",href:"/whg-training-resources/genome_wide_association_studies/meta-analysis_and_fine-mapping/Fine-mapping"},"Forward to the fine-mapping section")),(0,s.kt)("h3",{id:"conducting-fixed-effect-meta-analysis"},"Conducting fixed-effect meta-analysis"),(0,s.kt)("p",null,"The simplest way to perform meta-analysis is as follows.  We assume:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"that the ",(0,s.kt)("em",{parentName:"li"},"true effect")," is the same in both studies."),(0,s.kt)("li",{parentName:"ul"},"that the ",(0,s.kt)("em",{parentName:"li"},"observed effect")," in each study is equal to the ",(0,s.kt)("em",{parentName:"li"},"true effect")," plus noise.  (The noise is given by the study standard error, and is assumed to be gaussian in the usual way.)")),(0,s.kt)("p",null,"The full name of this is ",(0,s.kt)("em",{parentName:"p"},"inverse variance weighted fixed-effect meta-analysis"),".  "),(0,s.kt)("p",null,"Here's how it works: form a weighted average of the effect estimates, weighted by the inverse of the variances:"),(0,s.kt)("p",null,"$$ b = \\sum_i \\frac{\\beta_i}{\\text{se}_i^2} $$"),(0,s.kt)("p",null,"Compute also the sum of weights:"),(0,s.kt)("p",null,"$$ w = \\sum_i \\frac{1}{\\text{se}_i^2} $$"),(0,s.kt)("p",null,"Then the ",(0,s.kt)("em",{parentName:"p"},"meta-analysis estimate")," is"),(0,s.kt)("p",null,"$$ \\beta_{\\text{meta}} = b * w $$"),(0,s.kt)("p",null,"and the ",(0,s.kt)("em",{parentName:"p"},"meta-analysis standard error")," is"),(0,s.kt)("p",null,"$$ \\beta_{\\text{meta}} = \\frac{1}{\\sqrt{w}} $$"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Note.")," There are two good reasons to compute the estimate this way. Firstly, ",(0,s.kt)("a",{parentName:"p",href:"../../Statistical_modelling/notes/Normal%20times%20normal%20is%20normal.pdf"},"normal times\nnormal is\nnormal")," -\nand if you figure it out, you'll see the above calculation is the same as that lemma."),(0,s.kt)("p",null,"Secondly, it makes sense: the meta-analysis estimate is a weighted average of the per-study\nestimates, and they are weighted by the variance: studies with lots of uncertainty (large variance)\nget weighted down while studies with little uncertainty (small variance) get higher weight."),(0,s.kt)("h3",{id:"running-the-meta-analsis"},"Running the meta-analsis"),(0,s.kt)("p",null,"Here is a function to run the meta-analysis from our two studies:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"meta.analyse <- function( beta1, se1, beta2, se2 ) {\n  inverse.variances = c( 1/se1^2, 1/se2^2 )\n  b = c( beta1, beta2 ) * inverse.variances\n  meta.inverse.variance = sum( inverse.variances )\n  meta.beta = sum( b ) / meta.inverse.variance\n  meta.se = sqrt( 1 / meta.inverse.variance )\n  \n  # This formulation computes a two-tailed P-value\n  # as discussed in lectures\n  meta.P = pnorm( -abs( meta.beta ), sd = meta.se ) * 2\n  \n  # This formulation computes a Bayes factor under a N(0,0.2^2) prior\n  meta.BF = (\n    pnorm( meta.beta, mean = 0, sd = sqrt( meta.se^2 + 0.2 ) ) /\n    pnorm( meta.beta, mean = 0, sd = meta.se )\n  )\n\n  return( list(\n    meta.beta = meta.beta,\n    meta.se = meta.se,\n    meta.P = meta.P,\n    log10_meta.BF = log10( meta.BF )\n  ))\n}\n")),(0,s.kt)("p",null,"Try it on some fake data, for example:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"> meta.analyse( beta1 = 1, se1 = 0.1, beta2 = 1, se2 = 0.1 )\n")),(0,s.kt)("p",null,"Try a few different input values.  You should see:"),(0,s.kt)("ul",null,(0,s.kt)("li",{parentName:"ul"},"The ",(0,s.kt)("em",{parentName:"li"},"meta-analysis estimate")," should be somewhere between the two input estimates."),(0,s.kt)("li",{parentName:"ul"},"The ",(0,s.kt)("em",{parentName:"li"},"meta-analysis standard error")," should be smaller than either of the two study standard errors."),(0,s.kt)("li",{parentName:"ul"},"The ",(0,s.kt)("em",{parentName:"li"},"meta-analysis P-value")," depends on how well the two estimates 'stack up' on each other.  If they are both in the same direction and of roughly comparable size, the P-value will be lower than either of the study P-values.")),(0,s.kt)("p",null,"Here is a way to run the above function across all the data in our study."),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Note.")," We will use the ",(0,s.kt)("a",{parentName:"p",href:"https://purrr.tidyverse.org"},(0,s.kt)("inlineCode",{parentName:"a"},"purrr"))," function ",(0,s.kt)("inlineCode",{parentName:"p"},"map_dfr()")," to run the\nabove function across all rows and return a data frame. If you don't have ",(0,s.kt)("inlineCode",{parentName:"p"},"purrr"),", you could use a\nbase R function like ",(0,s.kt)("inlineCode",{parentName:"p"},"lapply()")," instead."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"meta_analysis = map_dfr(\n  1:nrow(study1),\n  function( i ) {\n    c(\n      rsid = study1$rsid[i],\n      chromosome = study1$chromosome[i],\n      position = study1$position[i],\n      allele1 = study1$allele1[i],\n      allele2 = study1$allele2[i],\n      study1.beta = study1$beta[i],\n      study1.se = study1$se[i],\n      study1.P = study1$P[i],\n      log10_study1.BF = study1$log10_BF[i],\n      study2.beta = study2$beta[i],\n      study2.se = study2$se[i],\n      study2.P = study2$P[i],\n      log10_study2.BF = study2$log10_BF[i],\n      meta.analyse(\n        study1$beta[i], study1$se[i],\n        study2$beta[i], study2$se[i]\n      )\n    )\n  }\n)\n")),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Question.")," What does the meta-analysis look like for the 'top' SNPs (e.g. those with the lowest\nP-values or highest Bayes factors) in study1 or 2? Is the evidence stronger or weaker across\nstudies? Has the meta-analysis changed the SNP or SNPs with the strongest evidence? Why?"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Hint.")," One way to pull out the 'top' SNps is to use the ",(0,s.kt)("inlineCode",{parentName:"p"},"order()")," function. For example to pull\nout the results for the ten SNPs with the lowest P-values in study 1, in order:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"meta_analysis[ order( meta_analysis$study1.P )[1:10], ]\n")),(0,s.kt)("h3",{id:"making-a-forest-plot"},"Making a forest plot"),(0,s.kt)("p",null,"To make best sense of this the tool for the job is a ",(0,s.kt)("a",{parentName:"p",href:"https://en.wikipedia.org/wiki/Forest_plot"},(0,s.kt)("em",{parentName:"a"},"forest plot")),".  We plot the study estimates and the meta-analysis estimate, along with their confidence/credible intervals, on seperate lines.  Let's do that now.  First here's our utility function to make a blank canvas:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"blank.plot <- function( xlim = c( 0, 1 ), ylim = c( 0, 1 ), xlab = \"\", ylab = \"\" ) {\n  # this function plots a blank canvas\n  plot(\n    0, 0,\n    col = 'white', # draw points white\n    bty = 'n',     # no border\n    xaxt = 'n',    # no x axis\n    yaxt = 'n',    # no y axis\n    xlab = xlab,   # no x axis label\n    ylab = ylab,   # no x axis label\n    xlim = xlim,\n    ylim = ylim\n  )\n}\n")),(0,s.kt)("p",null,"Now let's make a general forest plot function:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"\ndraw.forest.plot <- function(\n  betas,\n  ses,\n  names\n) {\n  # y axis locations for the lines.\n  # We assume the meta-analysis will go on the last line, so we separate it slightly by putting it at 1/2\n  y = c( length(betas):2, 0.5 )\n  \n  # learn a good x axis range by going out 3 standard errors from each estimate:\n  xlim = c( min( betas - 3 * ses ), max( betas + 3 * ses ))\n  # Also let's make sure to include zero in our range\n  xlim[1] = min( xlim[1], 0 )\n  xlim[2] = max( xlim[2], 0 )\n\n  # expand the range slightly\n  xcentre = mean(xlim)\n  xlim[1] = xcentre + (xlim[1] - xcentre) * 1.1\n  xlim[2] = xcentre + (xlim[2] - xcentre) * 1.1\n\n  # Give ourselves a big left margin for the row labels\n  par( mar = c( 4.1, 6.1, 2.1, 2.1 ))\n  blank.plot(\n    xlim = xlim,\n    ylim = c( range(y) + c( -0.5, 0.5 )),\n    xlab = \"Estimate and 95% CI\"\n  )\n  \n  # Draw the intervals first so they don't draw over the points\n  segments(\n    x0 = betas - 1.96 * ses, x1 = betas + 1.96 * ses,\n    y0 = y, y1 = y,\n    col = 'grey'\n  )\n\n  # Now plot the estimates\n  points(\n    x = betas,\n    y = y,\n    col = 'black',\n    pch = 19\n  )\n\n  # ... and add labels.  We put them 10% further left than the leftmost point\n  # and we right-align them\n  text.x = xcentre + (xlim[1] - xcentre) * 1.1\n  text(\n    x = text.x,\n    y = y,\n    labels = names,\n    adj = 1, # right-adjust\n    xpd = NA # this means \"Allow text outside the plot area\"\n  )\n  \n  # Add an x axis\n  axis( 1 )\n  \n  # add grid lines\n  grid()\n  \n  # add a solid line at 0\n  abline( v = 0, col = rgb( 0, 0, 0, 0.2 ), lwd = 2 )\n}\n")),(0,s.kt)("p",null,"For example, we could now make a forest plot for the first SNP:"),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'  forest.plot <- function( row ) {\n    betas = c( row$study1.beta, row$study2.beta, row$meta.beta )\n    ses = c( row$study1.se, row$study2.se, row$meta.se )\n    names = c( "Study 1", "Study 2", "Meta-analysis" )\n    draw.forest.plot( betas, ses, names )\n  }\n  \n  forest.plot( meta_analysis[1,] )\n')),(0,s.kt)("p",null,(0,s.kt)("img",{alt:"img",src:a(6051).Z,width:"400",height:"300"}),"\n",(0,s.kt)("strong",{parentName:"p"},"Question.")," What does the forest plot look like for the 'top' SNPs, i.e. for those with the lowest P-values (or highest Bayes factors)?  Plot a few of them and look at them.  Make sure you understand how the data in the meta-analysis file corresponds to the data on the plot."),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Note.")," Forest plots are deceptively simple - they don't look like there's much going on, but whenever I try to draw one I find it takes quite a bit of code.  (Like the ",(0,s.kt)("inlineCode",{parentName:"p"},"draw.forest.plot()")," function above, which already has ~40 lines of code).  This is typical of visualisation in general - the code always gets lengthy - and I think it is not really surprising.  It is because in a visualistion you are trying to convey a great deal of information clearly in a small space: this often takes great deal of careful tweaking to get right.  Don't skimp on this!"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Challenge."),' For a "working" plot, one thing it might be good to add to the plot would be more text listing the numerical values (i.e. the estimate and 95% confidence intervals)  E.g. this code:'),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},'labels = sprintf(\n  "%.2f (%.2f - %.2f)",\n  betas,\n  betas - 1.96 * ses,\n  betas + 1.96 * ses\n)\n')),(0,s.kt)("p",null,"produces the right kind of text.  Then you can add it to the plot in the right place."),(0,s.kt)("h3",{id:"other-meta-analysis-models"},"Other meta-analysis models"),(0,s.kt)("p",null,"The above is a simple meta-analysis that assumes that the true effect is the same in both studies.\nHowever, as we saw in the lecture, more complex models are also possible, including a 'random' effects analysis (that assumes that true effects are normally distributed around some mean) and include a heterogeneity test.  Here is a way to do this"),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Note.")," You will need the ",(0,s.kt)("inlineCode",{parentName:"p"},"meta")," library installed first - run ",(0,s.kt)("inlineCode",{parentName:"p"},'install.packages("meta")')," or use the ",(0,s.kt)("inlineCode",{parentName:"p"},"Tools")," -> ",(0,s.kt)("inlineCode",{parentName:"p"},"Install Packages")," menu in Rstudio."),(0,s.kt)("pre",null,(0,s.kt)("code",{parentName:"pre"},"library(meta)\nforest.meta(\n  metagen(\n    c( study1$beta[1], study2$beta[1] ),\n    c( study1$se[1],study2$se[1] )\n  )\n)\n")),(0,s.kt)("p",null,(0,s.kt)("strong",{parentName:"p"},"Question.")," What does this look like for the most-associated SNPs?  Is there any evidence of differences in effect between studies?"),(0,s.kt)("h3",{id:"fine-mapping-the-association"},"Fine-mapping the association"),(0,s.kt)("p",null,"By now you should have a good sense of which SNPs have lots of evidence for association in the two studies - and maybe these are the 'causal' SNPs.  However, an obvious possibility (if the gene is relevant for the disease) is that there could be multiple causal SNPs.  In the ",(0,s.kt)("a",{parentName:"p",href:"/whg-training-resources/genome_wide_association_studies/meta-analysis_and_fine-mapping/Fine-mapping"},"next section")," we will see one way to try to discover how many causal SNPs there are - and what they are."))}d.isMDXComponent=!0},6051:(e,t,a)=>{a.d(t,{Z:()=>n});const n=a.p+"assets/images/forest_plot_1st_SNP-cac3aaf427b2a427fb43946ff64bb08b.png"}}]);